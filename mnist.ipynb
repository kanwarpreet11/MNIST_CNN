{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is divided in two parts. In the first part we use Keras to understand, build and train a Deep Neural Network model. In the second part, we experiment with training and improving the Deep Learning model.\n",
    "\n",
    "#### need to be installed\n",
    "* [Keras](https://keras.io)\n",
    "* [Tensorflow](https://www.tensorflow.org/install/)\n",
    "* ```sklearn```, ```matplotlib``` and ```numpy``` also need to be installed\n",
    "\n",
    "Since Tensorflow 2.0, Keras is included in Tensorflow and will be automatically installed with Tensorflow. It can be accessed as ```tensorflow.keras```\n",
    "\n",
    "\n",
    "## Part 1\n",
    "\n",
    "### Losses\n",
    "The purpose of loss functions is to compute the quantity that a model should seek to minimize during training.\n",
    "https://keras.io/api/losses/\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "Optimization Algoritms are used to update the parameters of a model (weights and biases) to reduce the losses.\n",
    "See a list of available optimizers here - https://keras.io/api/optimizers/\n",
    "\n",
    "For a more complete overview of optimization algorithms see [this comparison](http://ruder.io/optimizing-gradient-descent/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "We use the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) database here. MNIST is a database of handwritten digits with 784 features, size-normalized and centered in 28x28 grayscale images. The datasets X and y are N-Dimensional array type. X is a dataset array of first 1000 images from MNIST. y is a dataset array of labels for these images.\n",
    "    \n",
    "```X = X.reshape(X.shape[0], 28, 28, 1)``` is used to reshape X to represent the 1000 images in a 4D array. 28x28 pixels in 1 channel, greyscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# for the random seed\n",
    "import tensorflow as tf\n",
    "\n",
    "# set the random seeds to get reproducible results\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X, y = X[:1000], y[:1000]\n",
    "X = X.reshape(X.shape[0], 28, 28, 1)\n",
    "# Normalize\n",
    "X = X / 255.\n",
    "# number of unique classes\n",
    "num_classes = len(np.unique(y))\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "\n",
    "num_tot = y.shape[0]\n",
    "num_train = y_train.shape[0]\n",
    "num_test = y_test.shape[0]\n",
    "\n",
    "y_oh = np.zeros((num_tot, num_classes))\n",
    "y_oh[range(num_tot), y] = 1\n",
    "\n",
    "y_oh_train = np.zeros((num_train, num_classes))\n",
    "y_oh_train[range(num_train), y_train] = 1\n",
    "\n",
    "y_oh_test = np.zeros((num_test, num_classes))\n",
    "y_oh_test[range(num_test), y_test] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check type and shape of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y is a labeled categorical dataset array and y_oh is an array consisting of dummy variables created for the y dataset.\n",
    "    \n",
    "One-hot encoding is used in y_oh, to convert the categorical variable y to numeric (containing different classes to be identified. Even though they are representing digits, here they are labels/classes and hence non-ordered values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of y:\n",
      " [5 0 4 1 9]\n",
      "First five rows of y_oh:\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print (f\"First five rows of y:\\n\", (y[0:5])) \n",
    "print (f\"First five rows of y_oh:\\n\", (y_oh[0:5])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data samples below along with the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#three data examples with labels\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.title(y[i])\n",
    "    plt.imshow(X[i])\n",
    "    plt.gray()\n",
    "    plt.axis('off') #we can remove this to see the image size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model\n",
    "\n",
    "We build a convolutional neural network ([CNN](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148), typically used for images recognition and classifications)\n",
    "    \n",
    "Dropout() is used to randonly drop neurons and their weights to avoid overfitting during training by learning lot of associations. It is a regularization technique used to ensure the model generalises well to new data.\n",
    "\n",
    "\n",
    "ReLU is used for activation on hidden layers and Softmax is used as an activation function for the output layer as it can handle multi-classification models like this one to give probability distribution for all classes, sum of which adds to 1. \n",
    "\n",
    "Categorical Crossentropy loss function is used. We start of by using the SGD optimizer. Weights are updated after every 32 data samples (batch_size=32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples\n",
      "Epoch 1/60\n",
      "800/800 [==============================] - 1s 655us/sample - loss: 2.1417\n",
      "Epoch 2/60\n",
      "800/800 [==============================] - 0s 267us/sample - loss: 1.0528\n",
      "Epoch 3/60\n",
      "800/800 [==============================] - 0s 266us/sample - loss: 0.5263\n",
      "Epoch 4/60\n",
      "800/800 [==============================] - 0s 240us/sample - loss: 0.3314\n",
      "Epoch 5/60\n",
      "800/800 [==============================] - 0s 241us/sample - loss: 0.2443\n",
      "Epoch 6/60\n",
      "800/800 [==============================] - 0s 233us/sample - loss: 0.2027\n",
      "Epoch 7/60\n",
      "800/800 [==============================] - 0s 230us/sample - loss: 0.1541\n",
      "Epoch 8/60\n",
      "800/800 [==============================] - 0s 233us/sample - loss: 0.1233\n",
      "Epoch 9/60\n",
      "800/800 [==============================] - 0s 255us/sample - loss: 0.0926\n",
      "Epoch 10/60\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 0.0741\n",
      "Epoch 11/60\n",
      "800/800 [==============================] - 0s 228us/sample - loss: 0.0494\n",
      "Epoch 12/60\n",
      "800/800 [==============================] - 0s 227us/sample - loss: 0.0435\n",
      "Epoch 13/60\n",
      "800/800 [==============================] - 0s 241us/sample - loss: 0.0274\n",
      "Epoch 14/60\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 0.0188\n",
      "Epoch 15/60\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 0.0149\n",
      "Epoch 16/60\n",
      "800/800 [==============================] - 0s 240us/sample - loss: 0.0103\n",
      "Epoch 17/60\n",
      "800/800 [==============================] - 0s 285us/sample - loss: 0.0081\n",
      "Epoch 18/60\n",
      "800/800 [==============================] - 0s 252us/sample - loss: 0.0057\n",
      "Epoch 19/60\n",
      "800/800 [==============================] - 0s 249us/sample - loss: 0.0053\n",
      "Epoch 20/60\n",
      "800/800 [==============================] - 0s 234us/sample - loss: 0.0041\n",
      "Epoch 21/60\n",
      "800/800 [==============================] - 0s 234us/sample - loss: 0.0036\n",
      "Epoch 22/60\n",
      "800/800 [==============================] - 0s 255us/sample - loss: 0.0032\n",
      "Epoch 23/60\n",
      "800/800 [==============================] - 0s 270us/sample - loss: 0.0029\n",
      "Epoch 24/60\n",
      "800/800 [==============================] - 0s 324us/sample - loss: 0.0027\n",
      "Epoch 25/60\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 0.0023\n",
      "Epoch 26/60\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.0023\n",
      "Epoch 27/60\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 0.0021\n",
      "Epoch 28/60\n",
      "800/800 [==============================] - 0s 283us/sample - loss: 0.0020\n",
      "Epoch 29/60\n",
      "800/800 [==============================] - 0s 252us/sample - loss: 0.0017\n",
      "Epoch 30/60\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.0017\n",
      "Epoch 31/60\n",
      "800/800 [==============================] - 0s 327us/sample - loss: 0.0015\n",
      "Epoch 32/60\n",
      "800/800 [==============================] - 0s 353us/sample - loss: 0.0014\n",
      "Epoch 33/60\n",
      "800/800 [==============================] - 0s 396us/sample - loss: 0.0014\n",
      "Epoch 34/60\n",
      "800/800 [==============================] - 0s 390us/sample - loss: 0.0014\n",
      "Epoch 35/60\n",
      "800/800 [==============================] - 0s 289us/sample - loss: 0.0013\n",
      "Epoch 36/60\n",
      "800/800 [==============================] - 0s 267us/sample - loss: 0.0012\n",
      "Epoch 37/60\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 0.0012\n",
      "Epoch 38/60\n",
      "800/800 [==============================] - 0s 244us/sample - loss: 0.0011\n",
      "Epoch 39/60\n",
      "800/800 [==============================] - 0s 245us/sample - loss: 0.0011\n",
      "Epoch 40/60\n",
      "800/800 [==============================] - 0s 265us/sample - loss: 0.0010\n",
      "Epoch 41/60\n",
      "800/800 [==============================] - 0s 260us/sample - loss: 9.6789e-04\n",
      "Epoch 42/60\n",
      "800/800 [==============================] - 0s 244us/sample - loss: 9.4181e-04\n",
      "Epoch 43/60\n",
      "800/800 [==============================] - 0s 253us/sample - loss: 9.4003e-04\n",
      "Epoch 44/60\n",
      "800/800 [==============================] - 0s 270us/sample - loss: 8.7250e-04\n",
      "Epoch 45/60\n",
      "800/800 [==============================] - 0s 279us/sample - loss: 8.3266e-04\n",
      "Epoch 46/60\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 8.0834e-04\n",
      "Epoch 47/60\n",
      "800/800 [==============================] - 0s 255us/sample - loss: 7.9508e-04\n",
      "Epoch 48/60\n",
      "800/800 [==============================] - 0s 291us/sample - loss: 7.6549e-04\n",
      "Epoch 49/60\n",
      "800/800 [==============================] - 0s 316us/sample - loss: 7.4577e-04\n",
      "Epoch 50/60\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 7.1445e-04\n",
      "Epoch 51/60\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 6.8757e-04\n",
      "Epoch 52/60\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 6.7677e-04\n",
      "Epoch 53/60\n",
      "800/800 [==============================] - 0s 244us/sample - loss: 6.4684e-04\n",
      "Epoch 54/60\n",
      "800/800 [==============================] - 0s 244us/sample - loss: 6.3878e-04\n",
      "Epoch 55/60\n",
      "800/800 [==============================] - 0s 242us/sample - loss: 6.1705e-04\n",
      "Epoch 56/60\n",
      "800/800 [==============================] - 0s 250us/sample - loss: 6.0446e-04\n",
      "Epoch 57/60\n",
      "800/800 [==============================] - 0s 245us/sample - loss: 5.8133e-04\n",
      "Epoch 58/60\n",
      "800/800 [==============================] - 0s 276us/sample - loss: 5.7128e-04\n",
      "Epoch 59/60\n",
      "800/800 [==============================] - 0s 285us/sample - loss: 5.6542e-04\n",
      "Epoch 60/60\n",
      "800/800 [==============================] - 0s 287us/sample - loss: 5.3972e-04\n",
      "200/200 [==============================] - 0s 548us/sample - loss: 0.4786\n",
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_oh_train, batch_size=32, epochs=60)\n",
    "\n",
    "# Evaluate performance\n",
    "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of parameters (weights and biases) in the model-\n",
    "\n",
    "    Conv2D Layer1: 3x3x1x16+1x16=160\n",
    "    Conv2D Layer2: 3x3x16x32+1x32=4,640\n",
    "    Dense Layer1: 800x128+1x128=102,528\n",
    "    Dense Layer2: 128x10+1x10=1,290\n",
    "    Total parameters in the model = 108,618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               102528    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 108,618\n",
      "Trainable params: 108,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Hyperparameter tuning\n",
    "\n",
    "A model's performance depends on many factors apart from the model architecture (e.g. type and number of layers) and the dataset. Any parameter that alters the learning of a model falls under hyperparameters. We can use various search methods to allow the algorithms to identify the best hyperparameters for the model, this process is called [hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization). It is best to use human judgement together with results from the hyperparameter search algorithms to arrive at the optimal parameters basis the task in hand and the resources available.\n",
    "\n",
    "We compare the training set loss with the loss on the test (/validation, read comments in the end) set to evaluate our model's learning performance.\n",
    "\n",
    "We look at the effect of a few hyperparameters here to illustrate this.\n",
    "\n",
    "We start by training the model on 150 epochs and visualizing the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/150\n",
      "800/800 [==============================] - 1s 770us/sample - loss: 2.1353 - val_loss: 1.7712\n",
      "Epoch 2/150\n",
      "800/800 [==============================] - 0s 364us/sample - loss: 1.0239 - val_loss: 0.6582\n",
      "Epoch 3/150\n",
      "800/800 [==============================] - 0s 316us/sample - loss: 0.5042 - val_loss: 0.4660\n",
      "Epoch 4/150\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 0.3721 - val_loss: 0.4062\n",
      "Epoch 5/150\n",
      "800/800 [==============================] - 0s 267us/sample - loss: 0.2711 - val_loss: 0.3992\n",
      "Epoch 6/150\n",
      "800/800 [==============================] - 0s 277us/sample - loss: 0.2135 - val_loss: 0.3133\n",
      "Epoch 7/150\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.1883 - val_loss: 0.3426\n",
      "Epoch 8/150\n",
      "800/800 [==============================] - 0s 331us/sample - loss: 0.1383 - val_loss: 0.3411\n",
      "Epoch 9/150\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 0.1049 - val_loss: 0.3401\n",
      "Epoch 10/150\n",
      "800/800 [==============================] - 0s 349us/sample - loss: 0.0895 - val_loss: 0.3321\n",
      "Epoch 11/150\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.0615 - val_loss: 0.3659\n",
      "Epoch 12/150\n",
      "800/800 [==============================] - 0s 287us/sample - loss: 0.0436 - val_loss: 0.3441\n",
      "Epoch 13/150\n",
      "800/800 [==============================] - 0s 364us/sample - loss: 0.0325 - val_loss: 0.3313\n",
      "Epoch 14/150\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 0.0277 - val_loss: 0.3720\n",
      "Epoch 15/150\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 0.0193 - val_loss: 0.3714\n",
      "Epoch 16/150\n",
      "800/800 [==============================] - 0s 343us/sample - loss: 0.0126 - val_loss: 0.3396\n",
      "Epoch 17/150\n",
      "800/800 [==============================] - 0s 282us/sample - loss: 0.0086 - val_loss: 0.3433\n",
      "Epoch 18/150\n",
      "800/800 [==============================] - 0s 349us/sample - loss: 0.0068 - val_loss: 0.3674\n",
      "Epoch 19/150\n",
      "800/800 [==============================] - 0s 336us/sample - loss: 0.0104 - val_loss: 0.3649\n",
      "Epoch 20/150\n",
      "800/800 [==============================] - 0s 364us/sample - loss: 0.0199 - val_loss: 0.3452\n",
      "Epoch 21/150\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 0.0172 - val_loss: 0.3624\n",
      "Epoch 22/150\n",
      "800/800 [==============================] - 0s 311us/sample - loss: 0.0102 - val_loss: 0.4054\n",
      "Epoch 23/150\n",
      "800/800 [==============================] - 0s 345us/sample - loss: 0.0053 - val_loss: 0.3503\n",
      "Epoch 24/150\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 0.0034 - val_loss: 0.3599\n",
      "Epoch 25/150\n",
      "800/800 [==============================] - 0s 331us/sample - loss: 0.0024 - val_loss: 0.3571\n",
      "Epoch 26/150\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 0.0021 - val_loss: 0.3649\n",
      "Epoch 27/150\n",
      "800/800 [==============================] - 0s 294us/sample - loss: 0.0018 - val_loss: 0.3651\n",
      "Epoch 28/150\n",
      "800/800 [==============================] - 0s 342us/sample - loss: 0.0016 - val_loss: 0.3658\n",
      "Epoch 29/150\n",
      "800/800 [==============================] - 0s 281us/sample - loss: 0.0015 - val_loss: 0.3765\n",
      "Epoch 30/150\n",
      "800/800 [==============================] - 0s 346us/sample - loss: 0.0014 - val_loss: 0.3723\n",
      "Epoch 31/150\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.0013 - val_loss: 0.3743\n",
      "Epoch 32/150\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.0013 - val_loss: 0.3787\n",
      "Epoch 33/150\n",
      "800/800 [==============================] - 0s 290us/sample - loss: 0.0012 - val_loss: 0.3829\n",
      "Epoch 34/150\n",
      "800/800 [==============================] - 0s 293us/sample - loss: 0.0011 - val_loss: 0.38326e-\n",
      "Epoch 35/150\n",
      "800/800 [==============================] - 0s 289us/sample - loss: 0.0011 - val_loss: 0.3839\n",
      "Epoch 36/150\n",
      "800/800 [==============================] - 0s 274us/sample - loss: 0.0010 - val_loss: 0.3867\n",
      "Epoch 37/150\n",
      "800/800 [==============================] - 0s 270us/sample - loss: 9.9187e-04 - val_loss: 0.3900\n",
      "Epoch 38/150\n",
      "800/800 [==============================] - 0s 275us/sample - loss: 9.3276e-04 - val_loss: 0.3900\n",
      "Epoch 39/150\n",
      "800/800 [==============================] - 0s 275us/sample - loss: 9.0335e-04 - val_loss: 0.3907\n",
      "Epoch 40/150\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 8.9234e-04 - val_loss: 0.3964\n",
      "Epoch 41/150\n",
      "800/800 [==============================] - 0s 326us/sample - loss: 8.4199e-04 - val_loss: 0.3966\n",
      "Epoch 42/150\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 8.0643e-04 - val_loss: 0.3980\n",
      "Epoch 43/150\n",
      "800/800 [==============================] - 0s 352us/sample - loss: 7.9066e-04 - val_loss: 0.4000\n",
      "Epoch 44/150\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 7.5162e-04 - val_loss: 0.4014\n",
      "Epoch 45/150\n",
      "800/800 [==============================] - 0s 327us/sample - loss: 7.1864e-04 - val_loss: 0.3996\n",
      "Epoch 46/150\n",
      "800/800 [==============================] - 0s 352us/sample - loss: 7.0299e-04 - val_loss: 0.4034\n",
      "Epoch 47/150\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 6.8729e-04 - val_loss: 0.4057\n",
      "Epoch 48/150\n",
      "800/800 [==============================] - 0s 334us/sample - loss: 6.5998e-04 - val_loss: 0.4055\n",
      "Epoch 49/150\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 6.4110e-04 - val_loss: 0.4066\n",
      "Epoch 50/150\n",
      "800/800 [==============================] - 0s 365us/sample - loss: 6.2151e-04 - val_loss: 0.4081\n",
      "Epoch 51/150\n",
      "800/800 [==============================] - 0s 343us/sample - loss: 6.0601e-04 - val_loss: 0.4118\n",
      "Epoch 52/150\n",
      "800/800 [==============================] - 0s 336us/sample - loss: 5.9633e-04 - val_loss: 0.4127\n",
      "Epoch 53/150\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 5.6967e-04 - val_loss: 0.4102\n",
      "Epoch 54/150\n",
      "800/800 [==============================] - 0s 374us/sample - loss: 5.6317e-04 - val_loss: 0.4134\n",
      "Epoch 55/150\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 5.4161e-04 - val_loss: 0.4132\n",
      "Epoch 56/150\n",
      "800/800 [==============================] - 0s 293us/sample - loss: 5.3337e-04 - val_loss: 0.4141\n",
      "Epoch 57/150\n",
      "800/800 [==============================] - 0s 379us/sample - loss: 5.1609e-04 - val_loss: 0.4162\n",
      "Epoch 58/150\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 5.0668e-04 - val_loss: 0.4188\n",
      "Epoch 59/150\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 4.9797e-04 - val_loss: 0.4185\n",
      "Epoch 60/150\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 4.7889e-04 - val_loss: 0.4191\n",
      "Epoch 61/150\n",
      "800/800 [==============================] - 0s 433us/sample - loss: 4.7352e-04 - val_loss: 0.4214\n",
      "Epoch 62/150\n",
      "800/800 [==============================] - 1s 630us/sample - loss: 4.6288e-04 - val_loss: 0.4212\n",
      "Epoch 63/150\n",
      "800/800 [==============================] - 0s 501us/sample - loss: 4.5326e-04 - val_loss: 0.4232\n",
      "Epoch 64/150\n",
      "800/800 [==============================] - 0s 425us/sample - loss: 4.4273e-04 - val_loss: 0.4218\n",
      "Epoch 65/150\n",
      "800/800 [==============================] - 0s 403us/sample - loss: 4.3409e-04 - val_loss: 0.4243\n",
      "Epoch 66/150\n",
      "800/800 [==============================] - 0s 275us/sample - loss: 4.2718e-04 - val_loss: 0.4246\n",
      "Epoch 67/150\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 4.1914e-04 - val_loss: 0.4273\n",
      "Epoch 68/150\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 4.0734e-04 - val_loss: 0.4254\n",
      "Epoch 69/150\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 3.9875e-04 - val_loss: 0.4255\n",
      "Epoch 70/150\n",
      "800/800 [==============================] - 0s 262us/sample - loss: 3.9330e-04 - val_loss: 0.4274\n",
      "Epoch 71/150\n",
      "800/800 [==============================] - 0s 287us/sample - loss: 3.9176e-04 - val_loss: 0.4303\n",
      "Epoch 72/150\n",
      "800/800 [==============================] - 0s 286us/sample - loss: 3.8035e-04 - val_loss: 0.4282\n",
      "Epoch 73/150\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 3.7193e-04 - val_loss: 0.4294\n",
      "Epoch 74/150\n",
      "800/800 [==============================] - 0s 336us/sample - loss: 3.6546e-04 - val_loss: 0.4308\n",
      "Epoch 75/150\n",
      "800/800 [==============================] - 0s 367us/sample - loss: 3.6222e-04 - val_loss: 0.4314\n",
      "Epoch 76/150\n",
      "800/800 [==============================] - 0s 273us/sample - loss: 3.5492e-04 - val_loss: 0.4332\n",
      "Epoch 77/150\n",
      "800/800 [==============================] - 0s 246us/sample - loss: 3.4777e-04 - val_loss: 0.4334\n",
      "Epoch 78/150\n",
      "800/800 [==============================] - 0s 246us/sample - loss: 3.4499e-04 - val_loss: 0.4334\n",
      "Epoch 79/150\n",
      "800/800 [==============================] - 0s 242us/sample - loss: 3.3301e-04 - val_loss: 0.4370\n",
      "Epoch 80/150\n",
      "800/800 [==============================] - 0s 247us/sample - loss: 3.2952e-04 - val_loss: 0.4346\n",
      "Epoch 81/150\n",
      "800/800 [==============================] - 0s 287us/sample - loss: 3.2406e-04 - val_loss: 0.4364\n",
      "Epoch 82/150\n",
      "800/800 [==============================] - 0s 265us/sample - loss: 3.1844e-04 - val_loss: 0.4378\n",
      "Epoch 83/150\n",
      "800/800 [==============================] - 0s 257us/sample - loss: 3.1315e-04 - val_loss: 0.4371\n",
      "Epoch 84/150\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 3.0870e-04 - val_loss: 0.4389\n",
      "Epoch 85/150\n",
      "800/800 [==============================] - 0s 258us/sample - loss: 3.0424e-04 - val_loss: 0.4387\n",
      "Epoch 86/150\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 2.9994e-04 - val_loss: 0.4396\n",
      "Epoch 87/150\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 2.9670e-04 - val_loss: 0.4400\n",
      "Epoch 88/150\n",
      "800/800 [==============================] - 0s 250us/sample - loss: 2.9030e-04 - val_loss: 0.4402\n",
      "Epoch 89/150\n",
      "800/800 [==============================] - 0s 248us/sample - loss: 2.8748e-04 - val_loss: 0.4422\n",
      "Epoch 90/150\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 2.8233e-04 - val_loss: 0.4424\n",
      "Epoch 91/150\n",
      "800/800 [==============================] - 0s 331us/sample - loss: 2.7931e-04 - val_loss: 0.4427\n",
      "Epoch 92/150\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 2.7342e-04 - val_loss: 0.4436\n",
      "Epoch 93/150\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 2.7243e-04 - val_loss: 0.4458\n",
      "Epoch 94/150\n",
      "800/800 [==============================] - 0s 373us/sample - loss: 2.6718e-04 - val_loss: 0.4442\n",
      "Epoch 95/150\n",
      "800/800 [==============================] - 0s 364us/sample - loss: 2.6585e-04 - val_loss: 0.4438\n",
      "Epoch 96/150\n",
      "800/800 [==============================] - 0s 336us/sample - loss: 2.6486e-04 - val_loss: 0.4458\n",
      "Epoch 97/150\n",
      "800/800 [==============================] - 0s 349us/sample - loss: 2.5775e-04 - val_loss: 0.4474\n",
      "Epoch 98/150\n",
      "800/800 [==============================] - 0s 276us/sample - loss: 2.5461e-04 - val_loss: 0.4475\n",
      "Epoch 99/150\n",
      "800/800 [==============================] - 0s 252us/sample - loss: 2.5170e-04 - val_loss: 0.4469\n",
      "Epoch 100/150\n",
      "800/800 [==============================] - 0s 282us/sample - loss: 2.4700e-04 - val_loss: 0.4472\n",
      "Epoch 101/150\n",
      "800/800 [==============================] - 0s 263us/sample - loss: 2.4404e-04 - val_loss: 0.4484\n",
      "Epoch 102/150\n",
      "800/800 [==============================] - 0s 241us/sample - loss: 2.4230e-04 - val_loss: 0.4489\n",
      "Epoch 103/150\n",
      "800/800 [==============================] - 0s 283us/sample - loss: 2.3778e-04 - val_loss: 0.4500\n",
      "Epoch 104/150\n",
      "800/800 [==============================] - 0s 320us/sample - loss: 2.3472e-04 - val_loss: 0.4502\n",
      "Epoch 105/150\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 2.3128e-04 - val_loss: 0.4499\n",
      "Epoch 106/150\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 2.3151e-04 - val_loss: 0.4505\n",
      "Epoch 107/150\n",
      "800/800 [==============================] - 0s 321us/sample - loss: 2.2667e-04 - val_loss: 0.4514\n",
      "Epoch 108/150\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 2.2453e-04 - val_loss: 0.4530\n",
      "Epoch 109/150\n",
      "800/800 [==============================] - 0s 362us/sample - loss: 2.2133e-04 - val_loss: 0.4534\n",
      "Epoch 110/150\n",
      "800/800 [==============================] - 0s 331us/sample - loss: 2.1864e-04 - val_loss: 0.4531\n",
      "Epoch 111/150\n",
      "800/800 [==============================] - 0s 278us/sample - loss: 2.1551e-04 - val_loss: 0.4547\n",
      "Epoch 112/150\n",
      "800/800 [==============================] - 0s 275us/sample - loss: 2.1531e-04 - val_loss: 0.4559\n",
      "Epoch 113/150\n",
      "800/800 [==============================] - 0s 316us/sample - loss: 2.1103e-04 - val_loss: 0.4543\n",
      "Epoch 114/150\n",
      "800/800 [==============================] - 0s 284us/sample - loss: 2.0914e-04 - val_loss: 0.4540\n",
      "Epoch 115/150\n",
      "800/800 [==============================] - 0s 322us/sample - loss: 2.0639e-04 - val_loss: 0.4558\n",
      "Epoch 116/150\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 2.0429e-04 - val_loss: 0.4565\n",
      "Epoch 117/150\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 2.0212e-04 - val_loss: 0.4572\n",
      "Epoch 118/150\n",
      "800/800 [==============================] - 0s 323us/sample - loss: 2.0083e-04 - val_loss: 0.4575\n",
      "Epoch 119/150\n",
      "800/800 [==============================] - 0s 270us/sample - loss: 1.9792e-04 - val_loss: 0.4583\n",
      "Epoch 120/150\n",
      "800/800 [==============================] - 0s 249us/sample - loss: 1.9648e-04 - val_loss: 0.4587\n",
      "Epoch 121/150\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 1.9384e-04 - val_loss: 0.4580\n",
      "Epoch 122/150\n",
      "800/800 [==============================] - 0s 245us/sample - loss: 1.9157e-04 - val_loss: 0.4589\n",
      "Epoch 123/150\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 1.9000e-04 - val_loss: 0.4590\n",
      "Epoch 124/150\n",
      "800/800 [==============================] - 0s 244us/sample - loss: 1.8762e-04 - val_loss: 0.4603\n",
      "Epoch 125/150\n",
      "800/800 [==============================] - 0s 245us/sample - loss: 1.8645e-04 - val_loss: 0.4600\n",
      "Epoch 126/150\n",
      "800/800 [==============================] - 0s 239us/sample - loss: 1.8402e-04 - val_loss: 0.4604\n",
      "Epoch 127/150\n",
      "800/800 [==============================] - 0s 237us/sample - loss: 1.8233e-04 - val_loss: 0.4617\n",
      "Epoch 128/150\n",
      "800/800 [==============================] - 0s 250us/sample - loss: 1.8096e-04 - val_loss: 0.4618\n",
      "Epoch 129/150\n",
      "800/800 [==============================] - 0s 325us/sample - loss: 1.7925e-04 - val_loss: 0.4614\n",
      "Epoch 130/150\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 1.7886e-04 - val_loss: 0.4627\n",
      "Epoch 131/150\n",
      "800/800 [==============================] - 0s 324us/sample - loss: 1.7567e-04 - val_loss: 0.4633\n",
      "Epoch 132/150\n",
      "800/800 [==============================] - 0s 339us/sample - loss: 1.7379e-04 - val_loss: 0.4629\n",
      "Epoch 133/150\n",
      "800/800 [==============================] - 0s 390us/sample - loss: 1.7212e-04 - val_loss: 0.4640\n",
      "Epoch 134/150\n",
      "800/800 [==============================] - 0s 311us/sample - loss: 1.7115e-04 - val_loss: 0.4645\n",
      "Epoch 135/150\n",
      "800/800 [==============================] - 0s 291us/sample - loss: 1.6880e-04 - val_loss: 0.4644\n",
      "Epoch 136/150\n",
      "800/800 [==============================] - 0s 311us/sample - loss: 1.6758e-04 - val_loss: 0.4652\n",
      "Epoch 137/150\n",
      "800/800 [==============================] - 0s 250us/sample - loss: 1.6597e-04 - val_loss: 0.4653\n",
      "Epoch 138/150\n",
      "800/800 [==============================] - 0s 253us/sample - loss: 1.6510e-04 - val_loss: 0.4655\n",
      "Epoch 139/150\n",
      "800/800 [==============================] - 0s 260us/sample - loss: 1.6317e-04 - val_loss: 0.4657\n",
      "Epoch 140/150\n",
      "800/800 [==============================] - 0s 263us/sample - loss: 1.6226e-04 - val_loss: 0.4668\n",
      "Epoch 141/150\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 1.6038e-04 - val_loss: 0.4675\n",
      "Epoch 142/150\n",
      "800/800 [==============================] - 0s 266us/sample - loss: 1.5896e-04 - val_loss: 0.4678\n",
      "Epoch 143/150\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 1.5704e-04 - val_loss: 0.4677\n",
      "Epoch 144/150\n",
      "800/800 [==============================] - 0s 250us/sample - loss: 1.5572e-04 - val_loss: 0.4678\n",
      "Epoch 145/150\n",
      "800/800 [==============================] - 0s 243us/sample - loss: 1.5461e-04 - val_loss: 0.4690\n",
      "Epoch 146/150\n",
      "800/800 [==============================] - 0s 319us/sample - loss: 1.5300e-04 - val_loss: 0.4695\n",
      "Epoch 147/150\n",
      "800/800 [==============================] - 0s 322us/sample - loss: 1.5197e-04 - val_loss: 0.4691\n",
      "Epoch 148/150\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 1.5103e-04 - val_loss: 0.4702\n",
      "Epoch 149/150\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 1.5046e-04 - val_loss: 0.4704\n",
      "Epoch 150/150\n",
      "800/800 [==============================] - 0s 408us/sample - loss: 1.4770e-04 - val_loss: 0.4705\n",
      "200/200 [==============================] - 0s 145us/sample - loss: 0.4705\n",
      "Accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.reset_states()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_oh_train, batch_size=32, epochs=150, validation_data = (X_test, y_oh_test))\n",
    "\n",
    "# Evaluate performance\n",
    "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of epochs for minimizing the test set loss seems to be around 10 epochs. The model was run multiple times to observe that it's typically around after 10 epochs that the test set loss is more. Though for accomodating variations (ups and downs), it's after around 20 epochs that the test loss starts to increases consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both a hyper-parameter search algorithm (similar to one shared ahead for dropout) and manual analysis was done to choose the below optimizers and for their learning rates (and also for momentum, decay etc. Though a constant learning rate gave the best results in each case), epochs etc. The algorithm has not been shared here (refer to the dropout search algorithm ahead for the code, note: these algorithms can be computationally heavy, especially grid search, as you can imagine, can take hours to identify the best parameters), below are the results by using the best identified optimizers and other best tuned parameters.\n",
    "    \n",
    "    Optimizers identified - Adam, Nadam and RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model using Adam (94.5% in this instance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/60\n",
      "800/800 [==============================] - 1s 928us/sample - loss: 1.3133 - val_loss: 0.5109\n",
      "Epoch 2/60\n",
      "800/800 [==============================] - 0s 366us/sample - loss: 0.3338 - val_loss: 0.2778\n",
      "Epoch 3/60\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 0.1393 - val_loss: 0.2311\n",
      "Epoch 4/60\n",
      "800/800 [==============================] - 0s 306us/sample - loss: 0.0855 - val_loss: 0.2839\n",
      "Epoch 5/60\n",
      "800/800 [==============================] - 0s 270us/sample - loss: 0.0727 - val_loss: 0.3065\n",
      "Epoch 6/60\n",
      "800/800 [==============================] - 0s 323us/sample - loss: 0.0331 - val_loss: 0.2973\n",
      "Epoch 7/60\n",
      "800/800 [==============================] - 0s 283us/sample - loss: 0.0333 - val_loss: 0.3325\n",
      "Epoch 8/60\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 0.0338 - val_loss: 0.3241\n",
      "Epoch 9/60\n",
      "800/800 [==============================] - 0s 253us/sample - loss: 0.0347 - val_loss: 0.3313\n",
      "Epoch 10/60\n",
      "800/800 [==============================] - 0s 261us/sample - loss: 0.0151 - val_loss: 0.4553\n",
      "Epoch 11/60\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 0.0117 - val_loss: 0.3637\n",
      "Epoch 12/60\n",
      "800/800 [==============================] - 0s 260us/sample - loss: 0.0679 - val_loss: 0.2950\n",
      "Epoch 13/60\n",
      "800/800 [==============================] - 0s 262us/sample - loss: 0.0583 - val_loss: 0.2172\n",
      "Epoch 14/60\n",
      "800/800 [==============================] - 0s 255us/sample - loss: 0.0289 - val_loss: 0.3875\n",
      "Epoch 15/60\n",
      "800/800 [==============================] - 0s 257us/sample - loss: 0.0113 - val_loss: 0.5336\n",
      "Epoch 16/60\n",
      "800/800 [==============================] - 0s 255us/sample - loss: 0.0034 - val_loss: 0.4128\n",
      "Epoch 17/60\n",
      "800/800 [==============================] - 0s 263us/sample - loss: 5.3057e-04 - val_loss: 0.4072\n",
      "Epoch 18/60\n",
      "800/800 [==============================] - 0s 261us/sample - loss: 1.4536e-04 - val_loss: 0.3977\n",
      "Epoch 19/60\n",
      "800/800 [==============================] - 0s 255us/sample - loss: 7.1287e-05 - val_loss: 0.3959\n",
      "Epoch 20/60\n",
      "800/800 [==============================] - 0s 261us/sample - loss: 5.2782e-05 - val_loss: 0.3955\n",
      "Epoch 21/60\n",
      "800/800 [==============================] - 0s 256us/sample - loss: 4.5305e-05 - val_loss: 0.3949\n",
      "Epoch 22/60\n",
      "800/800 [==============================] - 0s 334us/sample - loss: 3.9975e-05 - val_loss: 0.3957\n",
      "Epoch 23/60\n",
      "800/800 [==============================] - 0s 330us/sample - loss: 3.5596e-05 - val_loss: 0.3959\n",
      "Epoch 24/60\n",
      "800/800 [==============================] - 0s 289us/sample - loss: 3.2689e-05 - val_loss: 0.3965\n",
      "Epoch 25/60\n",
      "800/800 [==============================] - 0s 292us/sample - loss: 2.9929e-05 - val_loss: 0.3973\n",
      "Epoch 26/60\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 2.7650e-05 - val_loss: 0.3975\n",
      "Epoch 27/60\n",
      "800/800 [==============================] - 0s 316us/sample - loss: 2.5738e-05 - val_loss: 0.3982\n",
      "Epoch 28/60\n",
      "800/800 [==============================] - 0s 263us/sample - loss: 2.4019e-05 - val_loss: 0.3987\n",
      "Epoch 29/60\n",
      "800/800 [==============================] - 0s 291us/sample - loss: 2.2685e-05 - val_loss: 0.3992\n",
      "Epoch 30/60\n",
      "800/800 [==============================] - 0s 277us/sample - loss: 2.1304e-05 - val_loss: 0.3999\n",
      "Epoch 31/60\n",
      "800/800 [==============================] - 0s 255us/sample - loss: 2.0032e-05 - val_loss: 0.4002\n",
      "Epoch 32/60\n",
      "800/800 [==============================] - 0s 253us/sample - loss: 1.9114e-05 - val_loss: 0.4008\n",
      "Epoch 33/60\n",
      "800/800 [==============================] - 0s 256us/sample - loss: 1.8108e-05 - val_loss: 0.4016\n",
      "Epoch 34/60\n",
      "800/800 [==============================] - 0s 258us/sample - loss: 1.7143e-05 - val_loss: 0.4018\n",
      "Epoch 35/60\n",
      "800/800 [==============================] - 0s 336us/sample - loss: 1.6342e-05 - val_loss: 0.4023\n",
      "Epoch 36/60\n",
      "800/800 [==============================] - 0s 296us/sample - loss: 1.5627e-05 - val_loss: 0.4029\n",
      "Epoch 37/60\n",
      "800/800 [==============================] - 0s 342us/sample - loss: 1.4881e-05 - val_loss: 0.4033\n",
      "Epoch 38/60\n",
      "800/800 [==============================] - 0s 300us/sample - loss: 1.4271e-05 - val_loss: 0.4038\n",
      "Epoch 39/60\n",
      "800/800 [==============================] - 0s 311us/sample - loss: 1.3665e-05 - val_loss: 0.4043\n",
      "Epoch 40/60\n",
      "800/800 [==============================] - 0s 290us/sample - loss: 1.3085e-05 - val_loss: 0.4047\n",
      "Epoch 41/60\n",
      "800/800 [==============================] - 0s 262us/sample - loss: 1.2571e-05 - val_loss: 0.4051\n",
      "Epoch 42/60\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 1.2070e-05 - val_loss: 0.4056\n",
      "Epoch 43/60\n",
      "800/800 [==============================] - 0s 311us/sample - loss: 1.1629e-05 - val_loss: 0.4062\n",
      "Epoch 44/60\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 1.1173e-05 - val_loss: 0.4067\n",
      "Epoch 45/60\n",
      "800/800 [==============================] - 0s 274us/sample - loss: 1.0763e-05 - val_loss: 0.4070\n",
      "Epoch 46/60\n",
      "800/800 [==============================] - 0s 338us/sample - loss: 1.0379e-05 - val_loss: 0.4075\n",
      "Epoch 47/60\n",
      "800/800 [==============================] - 0s 267us/sample - loss: 9.9982e-06 - val_loss: 0.4081\n",
      "Epoch 48/60\n",
      "800/800 [==============================] - 0s 349us/sample - loss: 9.6704e-06 - val_loss: 0.4086\n",
      "Epoch 49/60\n",
      "800/800 [==============================] - 0s 348us/sample - loss: 9.3385e-06 - val_loss: 0.4091\n",
      "Epoch 50/60\n",
      "800/800 [==============================] - 0s 320us/sample - loss: 9.0192e-06 - val_loss: 0.4094\n",
      "Epoch 51/60\n",
      "800/800 [==============================] - 0s 282us/sample - loss: 8.7239e-06 - val_loss: 0.4099\n",
      "Epoch 52/60\n",
      "800/800 [==============================] - 0s 336us/sample - loss: 8.4365e-06 - val_loss: 0.4104\n",
      "Epoch 53/60\n",
      "800/800 [==============================] - 0s 324us/sample - loss: 8.1625e-06 - val_loss: 0.4109\n",
      "Epoch 54/60\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 7.9077e-06 - val_loss: 0.4112\n",
      "Epoch 55/60\n",
      "800/800 [==============================] - 0s 343us/sample - loss: 7.6759e-06 - val_loss: 0.4119\n",
      "Epoch 56/60\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 7.4354e-06 - val_loss: 0.4123\n",
      "Epoch 57/60\n",
      "800/800 [==============================] - 0s 349us/sample - loss: 7.2057e-06 - val_loss: 0.4126\n",
      "Epoch 58/60\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 6.9927e-06 - val_loss: 0.4132\n",
      "Epoch 59/60\n",
      "800/800 [==============================] - 0s 257us/sample - loss: 6.7836e-06 - val_loss: 0.4137\n",
      "Epoch 60/60\n",
      "800/800 [==============================] - 0s 252us/sample - loss: 6.5838e-06 - val_loss: 0.4142\n",
      "200/200 [==============================] - 0s 98us/sample - loss: 0.4142\n",
      "Accuracy: 0.945\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.reset_states()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_oh_train, batch_size=32, epochs=60, validation_data = (X_test, y_oh_test))\n",
    "\n",
    "# Evaluate performance\n",
    "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model using Nadam (96.5% in this instance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/60\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.3028 - val_loss: 0.4369\n",
      "Epoch 2/60\n",
      "800/800 [==============================] - 0s 293us/sample - loss: 0.2769 - val_loss: 0.3029\n",
      "Epoch 3/60\n",
      "800/800 [==============================] - 0s 291us/sample - loss: 0.1184 - val_loss: 0.1914\n",
      "Epoch 4/60\n",
      "800/800 [==============================] - 0s 271us/sample - loss: 0.0467 - val_loss: 0.3421\n",
      "Epoch 5/60\n",
      "800/800 [==============================] - 0s 291us/sample - loss: 0.0436 - val_loss: 0.1930\n",
      "Epoch 6/60\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 0.0094 - val_loss: 0.2371\n",
      "Epoch 7/60\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 0.0014 - val_loss: 0.2332\n",
      "Epoch 8/60\n",
      "800/800 [==============================] - 0s 389us/sample - loss: 3.7785e-04 - val_loss: 0.2436\n",
      "Epoch 9/60\n",
      "800/800 [==============================] - 0s 296us/sample - loss: 2.6164e-04 - val_loss: 0.2497\n",
      "Epoch 10/60\n",
      "800/800 [==============================] - 0s 268us/sample - loss: 2.0262e-04 - val_loss: 0.2561\n",
      "Epoch 11/60\n",
      "800/800 [==============================] - 0s 271us/sample - loss: 1.6553e-04 - val_loss: 0.2617\n",
      "Epoch 12/60\n",
      "800/800 [==============================] - 0s 290us/sample - loss: 1.3810e-04 - val_loss: 0.2662\n",
      "Epoch 13/60\n",
      "800/800 [==============================] - 0s 270us/sample - loss: 1.1847e-04 - val_loss: 0.2706\n",
      "Epoch 14/60\n",
      "800/800 [==============================] - 0s 360us/sample - loss: 1.0202e-04 - val_loss: 0.2744\n",
      "Epoch 15/60\n",
      "800/800 [==============================] - 0s 281us/sample - loss: 8.9389e-05 - val_loss: 0.2787\n",
      "Epoch 16/60\n",
      "800/800 [==============================] - 0s 311us/sample - loss: 7.9435e-05 - val_loss: 0.2821\n",
      "Epoch 17/60\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 7.0756e-05 - val_loss: 0.2853\n",
      "Epoch 18/60\n",
      "800/800 [==============================] - 0s 288us/sample - loss: 6.3728e-05 - val_loss: 0.2887\n",
      "Epoch 19/60\n",
      "800/800 [==============================] - 0s 275us/sample - loss: 5.7708e-05 - val_loss: 0.2914\n",
      "Epoch 20/60\n",
      "800/800 [==============================] - 0s 270us/sample - loss: 5.2426e-05 - val_loss: 0.2940\n",
      "Epoch 21/60\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 4.8035e-05 - val_loss: 0.2964\n",
      "Epoch 22/60\n",
      "800/800 [==============================] - 0s 296us/sample - loss: 4.4122e-05 - val_loss: 0.2988\n",
      "Epoch 23/60\n",
      "800/800 [==============================] - 0s 353us/sample - loss: 4.0657e-05 - val_loss: 0.3015\n",
      "Epoch 24/60\n",
      "800/800 [==============================] - 0s 277us/sample - loss: 3.7679e-05 - val_loss: 0.3038\n",
      "Epoch 25/60\n",
      "800/800 [==============================] - 0s 322us/sample - loss: 3.5131e-05 - val_loss: 0.3060\n",
      "Epoch 26/60\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 3.2668e-05 - val_loss: 0.3080\n",
      "Epoch 27/60\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 3.0513e-05 - val_loss: 0.3101\n",
      "Epoch 28/60\n",
      "800/800 [==============================] - 0s 337us/sample - loss: 2.8544e-05 - val_loss: 0.3120\n",
      "Epoch 29/60\n",
      "800/800 [==============================] - 0s 326us/sample - loss: 2.6769e-05 - val_loss: 0.3140\n",
      "Epoch 30/60\n",
      "800/800 [==============================] - 0s 320us/sample - loss: 2.5186e-05 - val_loss: 0.3159\n",
      "Epoch 31/60\n",
      "800/800 [==============================] - 0s 343us/sample - loss: 2.3735e-05 - val_loss: 0.3178\n",
      "Epoch 32/60\n",
      "800/800 [==============================] - 0s 364us/sample - loss: 2.2406e-05 - val_loss: 0.3194\n",
      "Epoch 33/60\n",
      "800/800 [==============================] - 0s 355us/sample - loss: 2.1171e-05 - val_loss: 0.3214\n",
      "Epoch 34/60\n",
      "800/800 [==============================] - 0s 352us/sample - loss: 2.0074e-05 - val_loss: 0.3229\n",
      "Epoch 35/60\n",
      "800/800 [==============================] - 0s 316us/sample - loss: 1.9025e-05 - val_loss: 0.3248\n",
      "Epoch 36/60\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 1.8108e-05 - val_loss: 0.3263\n",
      "Epoch 37/60\n",
      "800/800 [==============================] - 0s 351us/sample - loss: 1.7207e-05 - val_loss: 0.3279\n",
      "Epoch 38/60\n",
      "800/800 [==============================] - 0s 348us/sample - loss: 1.6369e-05 - val_loss: 0.3295\n",
      "Epoch 39/60\n",
      "800/800 [==============================] - 0s 360us/sample - loss: 1.5624e-05 - val_loss: 0.3310\n",
      "Epoch 40/60\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 1.4904e-05 - val_loss: 0.3323\n",
      "Epoch 41/60\n",
      "800/800 [==============================] - 0s 276us/sample - loss: 1.4270e-05 - val_loss: 0.3337\n",
      "Epoch 42/60\n",
      "800/800 [==============================] - 0s 262us/sample - loss: 1.3671e-05 - val_loss: 0.3353\n",
      "Epoch 43/60\n",
      "800/800 [==============================] - 0s 265us/sample - loss: 1.3107e-05 - val_loss: 0.3367\n",
      "Epoch 44/60\n",
      "800/800 [==============================] - 0s 268us/sample - loss: 1.2537e-05 - val_loss: 0.3378\n",
      "Epoch 45/60\n",
      "800/800 [==============================] - 0s 285us/sample - loss: 1.2024e-05 - val_loss: 0.3391\n",
      "Epoch 46/60\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 1.1549e-05 - val_loss: 0.3405\n",
      "Epoch 47/60\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 1.1085e-05 - val_loss: 0.3418\n",
      "Epoch 48/60\n",
      "800/800 [==============================] - 0s 356us/sample - loss: 1.0671e-05 - val_loss: 0.3430\n",
      "Epoch 49/60\n",
      "800/800 [==============================] - 0s 393us/sample - loss: 1.0250e-05 - val_loss: 0.3442\n",
      "Epoch 50/60\n",
      "800/800 [==============================] - 0s 392us/sample - loss: 9.8698e-06 - val_loss: 0.3453\n",
      "Epoch 51/60\n",
      "800/800 [==============================] - 0s 377us/sample - loss: 9.5176e-06 - val_loss: 0.3467\n",
      "Epoch 52/60\n",
      "800/800 [==============================] - 0s 351us/sample - loss: 9.1618e-06 - val_loss: 0.3478\n",
      "Epoch 53/60\n",
      "800/800 [==============================] - 0s 289us/sample - loss: 8.8468e-06 - val_loss: 0.3488\n",
      "Epoch 54/60\n",
      "800/800 [==============================] - 0s 274us/sample - loss: 8.5339e-06 - val_loss: 0.3499\n",
      "Epoch 55/60\n",
      "800/800 [==============================] - 0s 267us/sample - loss: 8.2383e-06 - val_loss: 0.3511\n",
      "Epoch 56/60\n",
      "800/800 [==============================] - 0s 262us/sample - loss: 7.9526e-06 - val_loss: 0.3521\n",
      "Epoch 57/60\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 7.6796e-06 - val_loss: 0.3531\n",
      "Epoch 58/60\n",
      "800/800 [==============================] - 0s 263us/sample - loss: 7.4366e-06 - val_loss: 0.3542\n",
      "Epoch 59/60\n",
      "800/800 [==============================] - 0s 263us/sample - loss: 7.1925e-06 - val_loss: 0.3552\n",
      "Epoch 60/60\n",
      "800/800 [==============================] - 0s 259us/sample - loss: 6.9467e-06 - val_loss: 0.3563\n",
      "200/200 [==============================] - 0s 102us/sample - loss: 0.3563\n",
      "Accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.reset_states()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_oh_train, batch_size=32, epochs=60, validation_data = (X_test, y_oh_test))\n",
    "\n",
    "# Evaluate performance\n",
    "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model using RMSprop (95% in this instance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/60\n",
      "800/800 [==============================] - 1s 921us/sample - loss: 1.9221 - val_loss: 1.0334\n",
      "Epoch 2/60\n",
      "800/800 [==============================] - 0s 289us/sample - loss: 0.5308 - val_loss: 0.3617\n",
      "Epoch 3/60\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 0.2613 - val_loss: 0.3003\n",
      "Epoch 4/60\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 0.1369 - val_loss: 0.3440\n",
      "Epoch 5/60\n",
      "800/800 [==============================] - 0s 267us/sample - loss: 0.1161 - val_loss: 0.2749\n",
      "Epoch 6/60\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 0.0497 - val_loss: 0.8137\n",
      "Epoch 7/60\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.0603 - val_loss: 0.3225\n",
      "Epoch 8/60\n",
      "800/800 [==============================] - 0s 364us/sample - loss: 0.0228 - val_loss: 0.3393\n",
      "Epoch 9/60\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 0.0435 - val_loss: 0.2913\n",
      "Epoch 10/60\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 0.0327 - val_loss: 0.3395\n",
      "Epoch 11/60\n",
      "800/800 [==============================] - 0s 286us/sample - loss: 9.1514e-04 - val_loss: 0.3668\n",
      "Epoch 12/60\n",
      "800/800 [==============================] - 0s 278us/sample - loss: 8.0781e-05 - val_loss: 0.3707\n",
      "Epoch 13/60\n",
      "800/800 [==============================] - 0s 275us/sample - loss: 1.1630e-05 - val_loss: 0.4171\n",
      "Epoch 14/60\n",
      "800/800 [==============================] - 0s 280us/sample - loss: 3.5808e-06 - val_loss: 0.4439\n",
      "Epoch 15/60\n",
      "800/800 [==============================] - 0s 276us/sample - loss: 7.3656e-07 - val_loss: 0.4771\n",
      "Epoch 16/60\n",
      "800/800 [==============================] - 0s 263us/sample - loss: 2.2352e-07 - val_loss: 0.5195\n",
      "Epoch 17/60\n",
      "800/800 [==============================] - 0s 261us/sample - loss: 6.6459e-08 - val_loss: 0.5544\n",
      "Epoch 18/60\n",
      "800/800 [==============================] - 0s 258us/sample - loss: 2.0117e-08 - val_loss: 0.5702\n",
      "Epoch 19/60\n",
      "800/800 [==============================] - 0s 261us/sample - loss: 7.0035e-09 - val_loss: 0.5935\n",
      "Epoch 20/60\n",
      "800/800 [==============================] - 0s 288us/sample - loss: 1.4901e-09 - val_loss: 0.6022\n",
      "Epoch 21/60\n",
      "800/800 [==============================] - 0s 331us/sample - loss: 7.4506e-10 - val_loss: 0.6106\n",
      "Epoch 22/60\n",
      "800/800 [==============================] - 0s 339us/sample - loss: 1.4901e-10 - val_loss: 0.6078\n",
      "Epoch 23/60\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 2.9802e-10 - val_loss: 0.6082\n",
      "Epoch 24/60\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 1.4901e-10 - val_loss: 0.6173\n",
      "Epoch 25/60\n",
      "800/800 [==============================] - 0s 290us/sample - loss: 0.0000e+00 - val_loss: 0.6199\n",
      "Epoch 26/60\n",
      "800/800 [==============================] - 0s 291us/sample - loss: 0.0000e+00 - val_loss: 0.6262\n",
      "Epoch 27/60\n",
      "800/800 [==============================] - 0s 274us/sample - loss: 0.0000e+00 - val_loss: 0.6366\n",
      "Epoch 28/60\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 0.0000e+00 - val_loss: 0.6462\n",
      "Epoch 29/60\n",
      "800/800 [==============================] - 0s 339us/sample - loss: 0.0000e+00 - val_loss: 0.6568\n",
      "Epoch 30/60\n",
      "800/800 [==============================] - 0s 314us/sample - loss: 0.0000e+00 - val_loss: 0.6666\n",
      "Epoch 31/60\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 0.0000e+00 - val_loss: 0.6770\n",
      "Epoch 32/60\n",
      "800/800 [==============================] - 0s 317us/sample - loss: 0.0000e+00 - val_loss: 0.6887\n",
      "Epoch 33/60\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 0.0000e+00 - val_loss: 0.6985\n",
      "Epoch 34/60\n",
      "800/800 [==============================] - 0s 324us/sample - loss: 0.0000e+00 - val_loss: 0.7095\n",
      "Epoch 35/60\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 0.0000e+00 - val_loss: 0.7204\n",
      "Epoch 36/60\n",
      "800/800 [==============================] - 0s 339us/sample - loss: 0.0000e+00 - val_loss: 0.7317\n",
      "Epoch 37/60\n",
      "800/800 [==============================] - 0s 286us/sample - loss: 0.0000e+00 - val_loss: 0.7435\n",
      "Epoch 38/60\n",
      "800/800 [==============================] - 0s 337us/sample - loss: 0.0000e+00 - val_loss: 0.7552\n",
      "Epoch 39/60\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 0.0000e+00 - val_loss: 0.7684\n",
      "Epoch 40/60\n",
      "800/800 [==============================] - 0s 261us/sample - loss: 0.0000e+00 - val_loss: 0.7819\n",
      "Epoch 41/60\n",
      "800/800 [==============================] - 0s 262us/sample - loss: 0.0000e+00 - val_loss: 0.7969\n",
      "Epoch 42/60\n",
      "800/800 [==============================] - 0s 256us/sample - loss: 0.0000e+00 - val_loss: 0.8131\n",
      "Epoch 43/60\n",
      "800/800 [==============================] - 0s 255us/sample - loss: 0.0000e+00 - val_loss: 0.8299\n",
      "Epoch 44/60\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 2.9802e-10 - val_loss: 0.8341\n",
      "Epoch 45/60\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 0.0000e+00 - val_loss: 0.8478\n",
      "Epoch 46/60\n",
      "800/800 [==============================] - 0s 337us/sample - loss: 0.0000e+00 - val_loss: 0.8610\n",
      "Epoch 47/60\n",
      "800/800 [==============================] - 0s 331us/sample - loss: 0.0000e+00 - val_loss: 0.8761\n",
      "Epoch 48/60\n",
      "800/800 [==============================] - 0s 283us/sample - loss: 0.0000e+00 - val_loss: 0.8924\n",
      "Epoch 49/60\n",
      "800/800 [==============================] - 0s 257us/sample - loss: 0.0000e+00 - val_loss: 0.9120\n",
      "Epoch 50/60\n",
      "800/800 [==============================] - 0s 259us/sample - loss: 1.4901e-10 - val_loss: 0.9144\n",
      "Epoch 51/60\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 2.9802e-10 - val_loss: 0.9104\n",
      "Epoch 52/60\n",
      "800/800 [==============================] - 0s 280us/sample - loss: 0.0000e+00 - val_loss: 0.9281\n",
      "Epoch 53/60\n",
      "800/800 [==============================] - 0s 314us/sample - loss: 0.0000e+00 - val_loss: 0.9457\n",
      "Epoch 54/60\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.0000e+00 - val_loss: 0.9645\n",
      "Epoch 55/60\n",
      "800/800 [==============================] - 0s 336us/sample - loss: 0.0000e+00 - val_loss: 0.9876\n",
      "Epoch 56/60\n",
      "800/800 [==============================] - 0s 260us/sample - loss: 2.9802e-10 - val_loss: 1.0069\n",
      "Epoch 57/60\n",
      "800/800 [==============================] - 0s 364us/sample - loss: 1.2197 - val_loss: 0.8574\n",
      "Epoch 58/60\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 4.1613e-05 - val_loss: 0.8188\n",
      "Epoch 59/60\n",
      "800/800 [==============================] - 0s 359us/sample - loss: 2.2944e-06 - val_loss: 0.8108\n",
      "Epoch 60/60\n",
      "800/800 [==============================] - 0s 335us/sample - loss: 1.3493e-06 - val_loss: 0.8040\n",
      "200/200 [==============================] - 0s 103us/sample - loss: 0.8040\n",
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.reset_states()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_oh_train, batch_size=32, epochs=60, validation_data = (X_test, y_oh_test))\n",
    "\n",
    "# Evaluate performance\n",
    "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter search for best dropout results in: Best: 0.948748 using {'dropout_rate': <span style=\"color:red\"> **0.5** </span>} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.948748 using {'dropout_rate': 0.5}\n",
      "0.936236 (0.013409) with: {'dropout_rate': 0.0}\n",
      "0.926244 (0.012771) with: {'dropout_rate': 0.1}\n",
      "0.947467 (0.018740) with: {'dropout_rate': 0.2}\n",
      "0.939967 (0.025144) with: {'dropout_rate': 0.3}\n",
      "0.939962 (0.024602) with: {'dropout_rate': 0.4}\n",
      "0.948748 (0.007707) with: {'dropout_rate': 0.5}\n",
      "0.939995 (0.003156) with: {'dropout_rate': 0.6}\n",
      "0.936240 (0.008174) with: {'dropout_rate': 0.7}\n",
      "0.893732 (0.014595) with: {'dropout_rate': 0.8}\n",
      "0.196174 (0.091748) with: {'dropout_rate': 0.9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model(dropout_rate=0.0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.reset_states()\n",
    "\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    # Max pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    # Max pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=60, batch_size=32, verbose=0)\n",
    "# define the grid search parameter\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_oh_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model\n",
    "Based on hyperparameter search for various hyperparameters and for best optimizer, the final model was fine tuned as follows.\n",
    "This best model gave an accuracy of around 97% in this instance, compared to an average model which gives an accuracy of around 86%, whereas the worst model (with various hyperparameters tested, few examples- learning rate of 0.3 or with optimizer as Adadelta) the accuracy was only around 8%.\n",
    "\n",
    "Note: since fine tuning was done using results from the validation/test set (data was only split into 2 sets here- training and another set we called test set), we can say that the model has been to some extent trained on the test set as well. Hence, ideally we should split the data into 3 sets, training and validation for fine tuning the model and a final test (unseen-holdout) set should be used to check the actual accuracy on an unseen data set and to make sure we didn't overfit while doing the tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.8075 - val_loss: 0.9834\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 323us/sample - loss: 0.7493 - val_loss: 0.3716\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 287us/sample - loss: 0.4329 - val_loss: 0.2940\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.309 - 0s 290us/sample - loss: 0.3082 - val_loss: 0.3137\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 370us/sample - loss: 0.3499 - val_loss: 0.2485\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 360us/sample - loss: 0.2325 - val_loss: 0.2254\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 370us/sample - loss: 0.2493 - val_loss: 0.1838\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 343us/sample - loss: 0.1853 - val_loss: 0.2019\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.1846 - val_loss: 0.5974\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 324us/sample - loss: 0.2346 - val_loss: 0.1958\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 349us/sample - loss: 0.1626 - val_loss: 0.2012\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 334us/sample - loss: 0.1509 - val_loss: 0.2123\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 348us/sample - loss: 0.1512 - val_loss: 0.1920\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.1647 - val_loss: 0.2609\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 334us/sample - loss: 0.1464 - val_loss: 0.2393\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 289us/sample - loss: 0.1310 - val_loss: 0.2057\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 393us/sample - loss: 0.1652 - val_loss: 0.2207\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 288us/sample - loss: 0.1435 - val_loss: 0.2028\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 280us/sample - loss: 0.1286 - val_loss: 0.2290\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 314us/sample - loss: 0.1425 - val_loss: 0.2386\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 0.1607 - val_loss: 0.2068\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 282us/sample - loss: 0.1158 - val_loss: 0.2325\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 278us/sample - loss: 0.1185 - val_loss: 0.1745\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 282us/sample - loss: 0.0943 - val_loss: 0.2591\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 283us/sample - loss: 0.1001 - val_loss: 0.2670\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 327us/sample - loss: 0.1199 - val_loss: 0.2919\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 378us/sample - loss: 0.1446 - val_loss: 0.2543\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 379us/sample - loss: 0.1239 - val_loss: 0.2369\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 318us/sample - loss: 0.1083 - val_loss: 0.2537\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 364us/sample - loss: 0.1340 - val_loss: 0.2993\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 322us/sample - loss: 0.1279 - val_loss: 0.2972\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 355us/sample - loss: 0.1453 - val_loss: 0.2722\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.2291 - val_loss: 0.1724\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 285us/sample - loss: 0.1152 - val_loss: 0.1572\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 276us/sample - loss: 0.1220 - val_loss: 0.1580\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 276us/sample - loss: 0.1281 - val_loss: 0.1982\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 273us/sample - loss: 0.0776 - val_loss: 0.1826\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 389us/sample - loss: 0.1443 - val_loss: 0.2510\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 386us/sample - loss: 0.1753 - val_loss: 0.2212\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 358us/sample - loss: 0.1846 - val_loss: 0.1955\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 364us/sample - loss: 0.1060 - val_loss: 0.1945\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 395us/sample - loss: 0.1714 - val_loss: 0.2760\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 365us/sample - loss: 0.2069 - val_loss: 0.1565\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 350us/sample - loss: 0.1114 - val_loss: 0.2419\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 292us/sample - loss: 0.1345 - val_loss: 0.2083\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 0.1717 - val_loss: 0.3128\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 349us/sample - loss: 0.1163 - val_loss: 0.2553\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 0.1019 - val_loss: 0.1706\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 340us/sample - loss: 0.0850 - val_loss: 0.2517\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 349us/sample - loss: 0.0733 - val_loss: 0.1989\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 0.1989\n",
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.reset_states()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_oh_train, batch_size=32, epochs=50, validation_data = (X_test, y_oh_test))\n",
    "\n",
    "# Evaluate performance\n",
    "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
